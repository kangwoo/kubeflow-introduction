{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in /home/jovyan/.local/lib/python3.6/site-packages (3.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.13.0)\n",
      "Requirement already satisfied: dill in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (0.3.0)\n",
      "Requirement already satisfied: promise in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (19.3.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (3.11.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (2.22.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (0.21.2)\n",
      "Requirement already satisfied: absl-py in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (0.8.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.11.2)\n",
      "Requirement already satisfied: tqdm in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (4.45.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets) (44.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/jovyan/.local/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.6)\n",
      "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.51.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fairing-tfjob-pvc.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile fairing-tfjob-pvc.yaml\n",
    "kind: PersistentVolumeClaim\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: fairing-tfjob-data-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 100Mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistentvolumeclaim/fairing-tfjob-data-pvc unchanged\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f fairing-tfjob-pvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fairing:include-cell\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def build_and_compile_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "@tfds.decode.make_decoder(output_dtype=tf.float32)\n",
    "def decode_image(example, feature):\n",
    "    return tf.cast(feature.decode_example(example), dtype=tf.float32) / 255\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "    mnist = tfds.builder('mnist', data_dir='./mnist')\n",
    "    mnist.download_and_prepare()\n",
    "\n",
    "    mnist_train, mnist_test = mnist.as_dataset(\n",
    "        split=['train', 'test'],\n",
    "        decoders={'image': decode_image()},\n",
    "        as_supervised=True)\n",
    "    train_input_dataset = mnist_train.cache().repeat().shuffle(\n",
    "        buffer_size=50000).batch(BATCH_SIZE)\n",
    "    eval_input_dataset = mnist_test.cache().repeat().batch(BATCH_SIZE)\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    train_input_dataset = train_input_dataset.with_options(options)\n",
    "    eval_input_dataset = eval_input_dataset.with_options(options)\n",
    "\n",
    "    print(\"Training...\")\n",
    "\n",
    "    with strategy.scope():\n",
    "        multi_worker_model = build_and_compile_model()\n",
    "\n",
    "    num_train_examples = mnist.info.splits['train'].num_examples\n",
    "    train_steps = num_train_examples // BATCH_SIZE\n",
    "    train_epochs = 10\n",
    "\n",
    "    multi_worker_model.fit(train_input_dataset, epochs=train_epochs, steps_per_epoch=train_steps)\n",
    "\n",
    "    # Evaluate the model on test set\n",
    "    score = multi_worker_model.evaluate(eval_input_dataset, steps=10)\n",
    "    print('Test accuracy: ', score[1])\n",
    "\n",
    "    model_base_path = '/app/data/models'\n",
    "    version = 1\n",
    "    model_path = os.path.join(model_base_path, str(version))\n",
    "    \n",
    "    multi_worker_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM tensorflow/tensorflow:2.1.0-py3\n",
    "\n",
    "RUN pip install tensorflow-datasets==2.0.0\n",
    "\n",
    "WORKDIR /app\n",
    "COPY /app/fairing_notebook_cluster_tfjob.py /app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairing_run():\n",
    "    import uuid\n",
    "    from kubeflow import fairing\n",
    "    from kubeflow.fairing.kubernetes import utils as k8s_utils\n",
    "    from kubeflow.fairing.builders.cluster.minio_context import MinioContextSource\n",
    "    from kubeflow.fairing.preprocessors.converted_notebook import FilterIncludeCell\n",
    "        \n",
    "    CONTAINER_REGISTRY = 'kangwoo'\n",
    "\n",
    "    namespace = 'admin'\n",
    "    job_name = f'fairing-notebook-cluster-tfjob-{uuid.uuid4().hex[:4]}'\n",
    "\n",
    "    s3_endpoint = 'minio-service.kubeflow.svc.cluster.local:9000'\n",
    "    minio_endpoint = \"http://\"+s3_endpoint\n",
    "    minio_username = \"minio\"\n",
    "    minio_key = \"minio123\"\n",
    "    minio_region = \"us-east-1\"\n",
    "\n",
    "    minio_context_source = MinioContextSource(endpoint_url=minio_endpoint, minio_secret=minio_username, minio_secret_key=minio_key, region_name=minio_region)\n",
    "    fairing.config.set_preprocessor('notebook', notebook_preprocessor=FilterIncludeCell)\n",
    "    fairing.config.set_builder('cluster', registry=CONTAINER_REGISTRY, image_name=\"fairing-notebook-cluster-tfjob\", dockerfile_path=\"Dockerfile\",\n",
    "                           context_source=minio_context_source)\n",
    "\n",
    "    fairing.config.set_deployer('tfjob', namespace=namespace, job_name=job_name, cleanup=False, stream_log=True, \n",
    "                            worker_count=2,\n",
    "                            pod_spec_mutators=[k8s_utils.mounting_pvc(pvc_name='fairing-tfjob-data-pvc', pvc_mount_path='/app/data')])\n",
    "\n",
    "    fairing.config.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200501 01:31:45 config:134] Using preprocessor: <kubeflow.fairing.preprocessors.converted_notebook.ConvertNotebookPreprocessor object at 0x7face85aebe0>\n",
      "[I 200501 01:31:45 config:136] Using builder: <kubeflow.fairing.builders.cluster.cluster.ClusterBuilder object at 0x7facc93a99e8>\n",
      "[I 200501 01:31:45 config:138] Using deployer: <kubeflow.fairing.deployers.tfjob.tfjob.TfJob object at 0x7fac51f256d8>\n",
      "[I 200501 01:31:45 cluster:46] Building image using cluster builder.\n",
      "[I 200501 01:31:45 base:107] Creating docker context: /tmp/fairing_context_xbg3wfjm\n",
      "[I 200501 01:31:45 converted_notebook:127] Converting fairing_notebook_cluster_tfjob.ipynb to fairing_notebook_cluster_tfjob.py\n",
      "[W 200501 01:31:45 manager:296] Waiting for fairing-builder-428k5-wqf4b to start...\n",
      "[W 200501 01:31:45 manager:296] Waiting for fairing-builder-428k5-wqf4b to start...\n",
      "[W 200501 01:31:45 manager:296] Waiting for fairing-builder-428k5-wqf4b to start...\n",
      "[I 200501 01:31:47 manager:302] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0003] Resolved base name tensorflow/tensorflow:2.1.0-py3 to tensorflow/tensorflow:2.1.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0003] Resolved base name tensorflow/tensorflow:2.1.0-py3 to tensorflow/tensorflow:2.1.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0003] Downloading base image tensorflow/tensorflow:2.1.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0005] Error while retrieving image from cache: getting file info: stat /cache/sha256:14ec674cefd622aa9d45f07485500da254acaf8adfef80bd0f279db03c735689: no such file or directory\n",
      "\u001b[36mINFO\u001b[0m[0005] Downloading base image tensorflow/tensorflow:2.1.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0007] Built cross stage deps: map[]\n",
      "\u001b[36mINFO\u001b[0m[0007] Downloading base image tensorflow/tensorflow:2.1.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0008] Error while retrieving image from cache: getting file info: stat /cache/sha256:14ec674cefd622aa9d45f07485500da254acaf8adfef80bd0f279db03c735689: no such file or directory\n",
      "\u001b[36mINFO\u001b[0m[0008] Downloading base image tensorflow/tensorflow:2.1.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0010] Unpacking rootfs as cmd RUN pip install tensorflow-datasets==2.0.0 requires it.\n",
      "\u001b[36mINFO\u001b[0m[0059] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0082] RUN pip install tensorflow-datasets==2.0.0\n",
      "\u001b[36mINFO\u001b[0m[0082] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0082] args: [-c pip install tensorflow-datasets==2.0.0]\n",
      "Collecting tensorflow-datasets==2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/88/b9/74c219b0310b3df0ac60c4948c4191b9377b6b746615b176819533096fb5/tensorflow_datasets-2.0.0-py3-none-any.whl (3.1MB)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.0.0) (1.11.2)\n",
      "Collecting attrs>=18.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.0.0) (1.1.0)\n",
      "Collecting dill\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
      "Collecting future\n",
      "  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading https://files.pythonhosted.org/packages/57/12/213dc5982e45283591ee0cb535b08ff603200ba84643bbea0aaa2109ed7c/tensorflow_metadata-0.21.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.0.0) (3.11.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.0.0) (2.22.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.0.0) (1.18.1)\n",
      "Collecting tqdm\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\n",
      "Collecting promise\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.0.0) (0.9.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.0.0) (1.13.0)\n",
      "Collecting googleapis-common-protos\n",
      "  Downloading https://files.pythonhosted.org/packages/05/46/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf/googleapis-common-protos-1.51.0.tar.gz\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets==2.0.0) (44.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets==2.0.0) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow-datasets==2.0.0) (2.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets==2.0.0) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets==2.0.0) (3.0.4)\n",
      "Building wheels for collected packages: dill, future, promise, googleapis-common-protos\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=80833 sha256=f2c73f326312aa2e53170bef3bc2d2852af55a94f03264b835e887388834e60f\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=493275 sha256=e5ad80a886a1ac35d6fb0fc3d71fb30e1d785bccdd89c26543ca183ee5eae733\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-cp36-none-any.whl size=23950 sha256=39ea7a44483fecf003fa9087cdcb5b2ff9bbcb58c2fdd3b4558b123b80db95c9\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/49/34/c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\n",
      "  Building wheel for googleapis-common-protos (setup.py): started\n",
      "  Building wheel for googleapis-common-protos (setup.py): finished with status 'done'\n",
      "  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-cp36-none-any.whl size=74529 sha256=b17675fa88822e3fcfe9e9c9eebdd5ca54d7a096c1ab2e2cb7f00b7bdb347879\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f9/7f/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\n",
      "Successfully built dill future promise googleapis-common-protos\n",
      "Installing collected packages: attrs, dill, future, googleapis-common-protos, tensorflow-metadata, tqdm, promise, tensorflow-datasets\n",
      "Successfully installed attrs-19.3.0 dill-0.3.1.1 future-0.18.2 googleapis-common-protos-1.51.0 promise-2.3 tensorflow-datasets-2.0.0 tensorflow-metadata-0.21.2 tqdm-4.45.0\n",
      "WARNING: You are using pip version 19.3.1; however, version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0090] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0092] WORKDIR /app\n",
      "\u001b[36mINFO\u001b[0m[0092] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0092] Changed working directory to /app\n",
      "\u001b[36mINFO\u001b[0m[0092] Creating directory /app\n",
      "\u001b[36mINFO\u001b[0m[0092] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0092] Using files from context: [/kaniko/buildcontext/app/fairing_notebook_cluster_tfjob.py]\n",
      "\u001b[36mINFO\u001b[0m[0092] COPY /app/fairing_notebook_cluster_tfjob.py /app\n",
      "\u001b[36mINFO\u001b[0m[0092] Taking snapshot of files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 200501 01:33:47 job:101] The tfjob fairing-notebook-cluster-tfjob-3f70 launched.\n",
      "[W 200501 01:33:47 manager:296] Waiting for fairing-notebook-cluster-tfjob-3f70-worker-0 to start...\n",
      "[W 200501 01:33:47 manager:296] Waiting for fairing-notebook-cluster-tfjob-3f70-worker-0 to start...\n",
      "[W 200501 01:33:47 manager:296] Waiting for fairing-notebook-cluster-tfjob-3f70-worker-0 to start...\n",
      "[I 200501 01:33:59 manager:302] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-01 01:33:59.758060: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-05-01 01:33:59.758122: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-05-01 01:33:59.758130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2020-05-01 01:34:00.325597: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-05-01 01:34:00.325619: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-05-01 01:34:00.325650: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2020-05-01 01:34:00.325827: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-05-01 01:34:00.333306: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3599890000 Hz\n",
      "2020-05-01 01:34:00.333834: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5349220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-05-01 01:34:00.333859: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-05-01 01:34:00.340075: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job chief -> {0 -> fairing-notebook-cluster-tfjob-3f70-chief-0.admin.svc:2222}\n",
      "2020-05-01 01:34:00.340117: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> fairing-notebook-cluster-tfjob-3f70-worker-1.admin.svc:2222}\n",
      "2020-05-01 01:34:00.340635: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2222\n",
      "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead set\n",
      "data_dir=gs://tfds-data/datasets.\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00,  4.63 file/s]\n",
      "WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\n",
      "WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\n",
      "WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\n",
      "WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\n",
      "WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\n",
      "WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\n",
      "TensorFlow version:  2.1.0\n",
      "\u001b[1mDownloading and preparing dataset mnist (11.06 MiB) to ./mnist/mnist/3.0.0...\u001b[0m\n",
      "\n",
      "\u001b[1mDataset mnist downloaded and prepared to ./mnist/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Training...\n",
      "Train for 937 steps\n",
      "Epoch 1/10\n",
      " 58/937 [>.............................] - ETA: 1:11 - loss: 1.0826 - accuracy: 0.6997 1\n",
      "125/937 [===>..........................] - ETA: 32s - loss: 0.7635 - accuracy: 0.7868\n",
      "194/937 [=====>........................] - ETA: 20s - loss: 0.6397 - accuracy: 0.818\n",
      "263/937 [=======>......................] - ETA: 14s - loss: 0.5595 - accuracy: 0.840\n",
      "334/937 [=========>....................] - ETA: 10s - loss: 0.5075 - accuracy: 0.855\n",
      "404/937 [===========>..................] - ETA: 8s - loss: 0.4727 - accuracy: 0.8656\n",
      "480/937 [==============>...............] - ETA: 6s - loss: 0.4437 - accuracy: 0.873\n",
      "564/937 [=================>............] - ETA: 4s - loss: 0.4171 - accuracy: 0.881\n",
      "664/937 [===================>..........] - ETA: 3s - loss: 0.3949 - accuracy: 0.88\n",
      "753/937 [======================>.......] - ETA: 2s - loss: 0.3767 - accuracy: 0.89\n",
      "841/937 [=========================>....] - ETA: 1s - loss: 0.3596 - accuracy: 0.89\n",
      "924/937 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.90\n",
      "937/937 [==============================] - 8s 9ms/step - loss: 0.3423 - accuracy: 0.9018\n",
      "Epoch 2/10\n",
      " 88/937 [=>............................] - ETA: 3s - loss: 0.1908 - accuracy: 0.94\n",
      "166/937 [====>.........................] - ETA: 2s - loss: 0.1866 - accuracy: 0.94\n",
      "250/937 [=======>......................] - ETA: 2s - loss: 0.1800 - accuracy: 0.94\n",
      "335/937 [=========>....................] - ETA: 2s - loss: 0.1777 - accuracy: 0.94\n",
      "417/937 [============>.................] - ETA: 1s - loss: 0.1752 - accuracy: 0.94\n",
      "505/937 [===============>..............] - ETA: 1s - loss: 0.1712 - accuracy: 0.95\n",
      "593/937 [=================>............] - ETA: 1s - loss: 0.1708 - accuracy: 0.95\n",
      "671/937 [====================>.........] - ETA: 0s - loss: 0.1673 - accuracy: 0.95\n",
      "756/937 [=======================>......] - ETA: 0s - loss: 0.1651 - accuracy: 0.951\n",
      "839/937 [=========================>....] - ETA: 0s - loss: 0.1629 - accuracy: 0.952\n",
      "924/937 [============================>.] - ETA: 0s - loss: 0.1608 - accuracy: 0.953\n",
      "937/937 [==============================] - 3s 4ms/step - loss: 0.1605 - accuracy: 0.9530\n",
      "Epoch 3/10\n",
      " 81/937 [=>............................] - ETA: 3s - loss: 0.1411 - accuracy: 0.95\n",
      "165/937 [====>.........................] - ETA: 2s - loss: 0.1472 - accuracy: 0.95\n",
      "247/937 [======>.......................] - ETA: 2s - loss: 0.1421 - accuracy: 0.95\n",
      "330/937 [=========>....................] - ETA: 2s - loss: 0.1397 - accuracy: 0.95\n",
      "414/937 [============>.................] - ETA: 1s - loss: 0.1387 - accuracy: 0.95\n",
      "489/937 [==============>...............] - ETA: 1s - loss: 0.1368 - accuracy: 0.95\n",
      "574/937 [=================>............] - ETA: 1s - loss: 0.1358 - accuracy: 0.95\n",
      "663/937 [====================>.........] - ETA: 1s - loss: 0.1356 - accuracy: 0.96\n",
      "747/937 [======================>.......] - ETA: 0s - loss: 0.1315 - accuracy: 0.961\n",
      "830/937 [=========================>....] - ETA: 0s - loss: 0.1288 - accuracy: 0.962\n",
      "907/937 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.962\n",
      "937/937 [==============================] - 4s 4ms/step - loss: 0.1270 - accuracy: 0.9628\n",
      "Epoch 4/10\n",
      " 86/937 [=>............................] - ETA: 3s - loss: 0.1167 - accuracy: 0.97\n",
      "175/937 [====>.........................] - ETA: 2s - loss: 0.1159 - accuracy: 0.96\n",
      "258/937 [=======>......................] - ETA: 2s - loss: 0.1137 - accuracy: 0.96\n",
      "343/937 [=========>....................] - ETA: 2s - loss: 0.1101 - accuracy: 0.96\n",
      "423/937 [============>.................] - ETA: 1s - loss: 0.1085 - accuracy: 0.96\n",
      "500/937 [===============>..............] - ETA: 1s - loss: 0.1069 - accuracy: 0.97\n",
      "589/937 [=================>............] - ETA: 1s - loss: 0.1062 - accuracy: 0.96\n",
      "672/937 [====================>.........] - ETA: 0s - loss: 0.1051 - accuracy: 0.96\n",
      "758/937 [=======================>......] - ETA: 0s - loss: 0.1039 - accuracy: 0.969\n",
      "845/937 [==========================>...] - ETA: 0s - loss: 0.1036 - accuracy: 0.970\n",
      "929/937 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.970\n",
      "937/937 [==============================] - 3s 4ms/step - loss: 0.1032 - accuracy: 0.9703\n",
      "Epoch 5/10\n",
      " 87/937 [=>............................] - ETA: 3s - loss: 0.0797 - accuracy: 0.97\n",
      "167/937 [====>.........................] - ETA: 2s - loss: 0.0887 - accuracy: 0.97\n",
      "248/937 [======>.......................] - ETA: 2s - loss: 0.0892 - accuracy: 0.97\n",
      "330/937 [=========>....................] - ETA: 2s - loss: 0.0899 - accuracy: 0.97\n",
      "410/937 [============>.................] - ETA: 2s - loss: 0.0883 - accuracy: 0.97\n",
      "496/937 [==============>...............] - ETA: 1s - loss: 0.0858 - accuracy: 0.97\n",
      "580/937 [=================>............] - ETA: 1s - loss: 0.0853 - accuracy: 0.97\n",
      "663/937 [====================>.........] - ETA: 1s - loss: 0.0860 - accuracy: 0.97\n",
      "743/937 [======================>.......] - ETA: 0s - loss: 0.0860 - accuracy: 0.974\n",
      "824/937 [=========================>....] - ETA: 0s - loss: 0.0844 - accuracy: 0.975\n",
      "911/937 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.975\n",
      "937/937 [==============================] - 4s 4ms/step - loss: 0.0842 - accuracy: 0.9753\n",
      "Epoch 6/10\n",
      " 84/937 [\n",
      "166/937 [====>.....\n",
      "255/937 [=======>............\n",
      "337/937 [=========>....................\n",
      "421/937 [============>.................] - ETA: 1\n",
      "505/937 [===============>..............] - ETA: 1s - loss:\n",
      "582/937 [=================>............] - ETA: 1s - loss: 0.0749 - a\n",
      "664/937 [====================>.........] - ETA: 1s - loss: 0.0758 - accuracy: 0\n",
      "743/937 [======================>.......] - ETA: 0s - loss: 0.0754 - accuracy: 0.9761\b\b\b\b\b\n",
      "828/937 [=========================>....] - ETA: 0s - loss: 0.0747 - accuracy: 0.9765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "903/937 [===========================>..] - ETA: 0s - loss: 0.0748 - accuracy: 0.9763\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "937/937 [==============================] - 4s 4ms/step - loss: 0.0742 - accuracy: 0.9766\n",
      "Epoch 7/10\n",
      " 89/937 [\n",
      "176/937 [====>.....\n",
      "256/937 [=======>............\n",
      "331/937 [=========>....................\n",
      "416/937 [============>.................] - ETA: 1\n",
      "499/937 [==============>...............] - ETA: 1s - loss:\n",
      "585/937 [=================>............] - ETA: 1s - loss: 0.0698 - a\n",
      "669/937 [====================>.........] - ETA: 0s - loss: 0.0696 - accuracy: 0\n",
      "746/937 [======================>.......] - ETA: 0s - loss: 0.0690 - accuracy: 0.9780\b\b\b\b\b\n",
      "823/937 [=========================>....] - ETA: 0s - loss: 0.0689 - accuracy: 0.9783\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "907/937 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "937/937 [==============================] - 4s 4ms/step - loss: 0.0683 - accuracy: 0.9786\n",
      "Epoch 8/10\n",
      " 77/937 [\n",
      "159/937 [====>.....\n",
      "246/937 [======>.............\n",
      "334/937 [=========>....................\n",
      "415/937 [============>.................] - ETA: 1\n",
      "500/937 [===============>..............] - ETA: 1s - loss:\n",
      "584/937 [=================>............] - ETA: 1s - loss: 0.0592 - a\n",
      "664/937 [====================>.........] - ETA: 1s - loss: 0.0589 - accuracy: 0\n",
      "747/937 [======================>.......] - ETA: 0s - loss: 0.0581 - accuracy: 0.9816\b\b\b\b\b\n",
      "836/937 [=========================>....] - ETA: 0s - loss: 0.0584 - accuracy: 0.9814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "922/937 [============================>.] - ETA: 0s - loss: 0.0584 - accuracy: 0.9813\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "937/937 [==============================] - 3s 4ms/step - loss: 0.0581 - accuracy: 0.9814\n",
      "Epoch 9/10\n",
      " 82/937 [\n",
      "168/937 [====>.....\n",
      "249/937 [======>.............\n",
      "333/937 [=========>....................\n",
      "416/937 [============>.................] - ETA: 1\n",
      "497/937 [==============>...............] - ETA: 1s - loss:\n",
      "581/937 [=================>............] - ETA: 1s - loss: 0.0546 - a\n",
      "666/937 [====================>.........] - ETA: 1s - loss: 0.0541 - accuracy: 0\n",
      "755/937 [=======================>......] - ETA: 0s - loss: 0.0547 - accuracy: 0.9828\b\b\b\b\b\n",
      "839/937 [=========================>....] - ETA: 0s - loss: 0.0547 - accuracy: 0.9829\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "920/937 [============================>.] - ETA: 0s - loss: 0.0549 - accuracy: 0.9829\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "937/937 [==============================] - 4s 4ms/step - loss: 0.0550 - accuracy: 0.9829\n",
      "Epoch 10/10\n",
      " 86/937 [\n",
      "170/937 [====>.....\n",
      "251/937 [=======>............\n",
      "333/937 [=========>....................\n",
      "418/937 [============>.................] - ETA: 1\n",
      "502/937 [===============>..............] - ETA: 1s - loss:\n",
      "588/937 [=================>............] - ETA: 1s - loss: 0.0487 - a\n",
      "671/937 [====================>.........] - ETA: 0s - loss: 0.0483 - accuracy: 0\n",
      "757/937 [=======================>......] - ETA: 0s - loss: 0.0483 - accuracy: 0.9849\b\b\b\b\b\n",
      "840/937 [=========================>....] - ETA: 0s - loss: 0.0479 - accuracy: 0.9850\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "923/937 [============================>.] - ETA: 0s - loss: 0.0484 - accuracy: 0.9846\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "937/937 [==============================] - 3s 4ms/step - loss: 0.0485 - accuracy: 0.9846\n",
      "2020-05-01 01:34:47.914712: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\n",
      "WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\n",
      "WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\n",
      "WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\n",
      "WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\n",
      "WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.0253 - accuracy: 0.9860\n",
      "2020-05-01 01:34:49.101206: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2020-05-01 01:34:49.105000: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "2020-05-01 01:34:49.239207: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Test accuracy:  0.9859813\n",
      "2020-05-01 01:34:49.721109: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\n"
     ]
    }
   ],
   "source": [
    "#fairing:include-cell\n",
    "if __name__ == '__main__':\n",
    "    if os.getenv('FAIRING_RUNTIME', None) is None:\n",
    "        fairing_run()\n",
    "    else:\n",
    "        train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
